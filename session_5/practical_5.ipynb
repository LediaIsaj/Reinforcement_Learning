{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# [M2-AI-Univ. Paris Saclay] Direct Policy Search\n",
    "\n",
    "In this practical, you are asked to put what you just learnt\n",
    "about direct policy search. \n",
    "\n",
    "\n",
    "In this project, you are asked to solve the classic Mountain Car (https://gym.openai.com/envs/MountainCar-v0/). For more details about action and observation space, please refer to the OpenAI\n",
    "documentation here: https://github.com/openai/gym/wiki/MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1. Discrete Action Spaces\n",
    "\n",
    "You are expected to implement direct policy search algorithm using Black-Box optimization algoritms (evolutionary computation: CMA-ES, differential evolution: scipy.optimize). We are in the setting of model free approach.\n",
    "\n",
    "In order to efficienlty train your agent, you must (ref. page 58; Mich√®le's slides):\n",
    "* Define your search space (policy space in which your are willing to search for)\n",
    "* Define your objective function: to assess a policy (Episode-based or step based)\n",
    "* Optimize the objective using balck-box optimizer (cma-es: use https://pypi.org/project/cma/ ; differential evolution: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html)\n",
    "\n",
    "Complete Agent Class:\n",
    "1. `train` method: for optimizing the objective function to get optimal policy\n",
    "2. `act` method: use optimal policy to output action for each state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your import ?\n",
    "import cma\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Init a new agent.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn your policy.\n",
    "\n",
    "        Possible action: [0, 1, 2]\n",
    "        Range observation (tuple):\n",
    "            - position: [-1.2, 0.6]\n",
    "            - velocity: [-0.07, 0.07]\n",
    "        \"\"\"\n",
    "        # 1- Define state features\n",
    "        # 2- Define search space (to define a policy)\n",
    "        # 3- Define objective function (for policy evaluation)\n",
    "        # 4- Optimize the objective function\n",
    "        # 5- Save optimal policy\n",
    "\n",
    "        # This is an example\n",
    "        def objective_function(W):\n",
    "            total = 0\n",
    "            env = gym.make(\"MountainCar-v0\")\n",
    "            env.seed(np.random.randint(1000))\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.argmax(np.dot(state,W.reshape(2,3)))\n",
    "                state, reward, done, info = env.step(action)\n",
    "                total += -1\n",
    "            return - total # loss\n",
    "        \n",
    "        policy_opt, _ = cma.fmin2(objective_function, np.zeros(6), 0.5,restarts = 5)\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Acts given an observation of the environment (using learned policy).\n",
    "\n",
    "        Takes as argument an observation of the current state, and\n",
    "        returns the chosen action.\n",
    "        See environment documentation: https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "        Possible action: [0, 1, 2]\n",
    "        Range observation (tuple):\n",
    "            - position: [-1.2, 0.6]\n",
    "            - velocity: [-0.07, 0.07]\n",
    "        \"\"\"\n",
    "        return np.argmax(np.dot(state,self.policy_opt.reshape(2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4_w,9)-aCMA-ES (mu_w=2.8,w_1=49%) in dimension 6 (seed=614558, Wed Dec  9 14:03:46 2020)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1      9 2.000000000000000e+02 1.0e+00 4.52e-01  4e-01  5e-01 0:00.1\n",
      "termination on tolfun=1e-11 (Wed Dec  9 14:03:46 2020)\n",
      "final/bestever f-value = 2.000000e+02 2.000000e+02\n",
      "incumbent solution: [-0.05005723991971377, -0.021489270951024732, 0.0418083559108522, -0.11948201087685248, 0.3295870702103614, -0.43633720203811655]\n",
      "std deviation: [0.4328525860830529, 0.4371905857071624, 0.4634439214695943, 0.44631745359675545, 0.46310530961606555, 0.42793634714214385]\n",
      "(9_w,18)-aCMA-ES (mu_w=5.4,w_1=30%) in dimension 6 (seed=614559, Wed Dec  9 14:03:46 2020)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1     28 2.000000000000000e+02 1.0e+00 4.35e-01  4e-01  4e-01 0:00.3\n",
      "termination on tolfun=1e-11 after 1 restart (Wed Dec  9 14:03:47 2020)\n",
      "final/bestever f-value = 2.000000e+02 2.000000e+02\n",
      "incumbent solution: [0.20294392599407293, 0.007593599090616252, -0.007294181785848859, 0.1398230419991054, -0.002305722843373024, 0.23612968585312583]\n",
      "std deviation: [0.4293278625749058, 0.4297947937743799, 0.39990311497285624, 0.448631029393026, 0.3877277574339855, 0.44804602669595023]\n",
      "(18_w,36)-aCMA-ES (mu_w=10.2,w_1=18%) in dimension 6 (seed=614560, Wed Dec  9 14:03:47 2020)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1     65 1.960000000000000e+02 1.0e+00 4.66e-01  4e-01  5e-01 0:00.4\n",
      "    2    101 2.000000000000000e+02 1.2e+00 3.92e-01  3e-01  4e-01 0:00.8\n",
      "    3    137 2.000000000000000e+02 1.5e+00 3.36e-01  3e-01  4e-01 0:01.2\n",
      "termination on tolflatfitness=1 after 2 restarts (Wed Dec  9 14:03:48 2020)\n",
      "final/bestever f-value = 2.000000e+02 1.960000e+02\n",
      "incumbent solution: [-0.0029911693214989543, -0.10927051083221569, 0.02914860086479798, -0.12932868397594413, -0.1330032826966654, -0.017430023893917863]\n",
      "std deviation: [0.3178987594249223, 0.31156524685167314, 0.30706740787583187, 0.37882391631414936, 0.3005491349591547, 0.35217280403764323]\n",
      "(36_w,72)-aCMA-ES (mu_w=19.7,w_1=10%) in dimension 6 (seed=614561, Wed Dec  9 14:03:48 2020)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1    210 2.000000000000000e+02 1.0e+00 5.98e-01  6e-01  7e-01 0:00.8\n",
      "termination on tolfun=1e-11 after 3 restarts (Wed Dec  9 14:03:49 2020)\n",
      "final/bestever f-value = 2.000000e+02 1.960000e+02\n",
      "incumbent solution: [0.19368948239425698, 0.0016311533992773024, 0.11590987365008647, 0.0857624238374984, -0.33426154171581934, -0.22361490393051536]\n",
      "std deviation: [0.612031029724441, 0.5650972935315904, 0.6892272384027939, 0.6322011270413379, 0.6067969550348504, 0.6159710912948921]\n",
      "(72_w,144)-aCMA-ES (mu_w=38.3,w_1=6%) in dimension 6 (seed=614562, Wed Dec  9 14:03:49 2020)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1    355 1.740000000000000e+02 1.0e+00 5.33e-01  5e-01  6e-01 0:01.7\n",
      "    2    499 2.000000000000000e+02 1.4e+00 5.46e-01  5e-01  6e-01 0:03.2\n",
      "    3    643 1.590000000000000e+02 1.7e+00 5.20e-01  5e-01  6e-01 0:04.8\n",
      "    5    931 2.000000000000000e+02 2.0e+00 5.11e-01  4e-01  6e-01 0:08.4\n",
      "termination on tolflatfitness=1 after 4 restarts (Wed Dec  9 14:03:57 2020)\n",
      "final/bestever f-value = 2.000000e+02 1.590000e+02\n",
      "incumbent solution: [-0.08009397218593388, -0.09692151753783149, -0.37762973002108297, 0.2393990737461658, 0.05286189718411055, 0.2262706757981982]\n",
      "std deviation: [0.5622756502303496, 0.46219599367744996, 0.5037043224959434, 0.44888533017957777, 0.5461726414951672, 0.40721915090209426]\n",
      "(144_w,288)-aCMA-ES (mu_w=75.0,w_1=3%) in dimension 6 (seed=614563, Wed Dec  9 14:03:57 2020)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1   1220 1.130000000000000e+02 1.0e+00 5.19e-01  5e-01  6e-01 0:03.4\n",
      "    2   1508 1.630000000000000e+02 1.4e+00 5.49e-01  5e-01  6e-01 0:06.5\n",
      "    3   1796 1.220000000000000e+02 1.9e+00 6.16e-01  6e-01  7e-01 0:09.6\n",
      "    5   2372 1.210000000000000e+02 1.7e+00 7.18e-01  5e-01  9e-01 0:16.9\n",
      "    7   2948 1.160000000000000e+02 2.5e+00 7.72e-01  5e-01  1e+00 0:23.4\n",
      "    9   3524 1.160000000000000e+02 2.8e+00 7.47e-01  5e-01  1e+00 0:29.6\n",
      "   11   4100 1.150000000000000e+02 4.7e+00 7.72e-01  5e-01  1e+00 0:39.0\n",
      "   13   4676 1.150000000000000e+02 5.6e+00 7.38e-01  4e-01  1e+00 0:46.1\n",
      "   16   5540 1.150000000000000e+02 7.1e+00 8.88e-01  3e-01  2e+00 0:57.2\n",
      "   19   6404 1.150000000000000e+02 1.7e+01 1.28e+00  3e-01  3e+00 1:07.2\n",
      "   22   7268 1.130000000000000e+02 7.2e+01 1.48e+00  3e-01  3e+00 1:17.6\n",
      "   26   8420 1.130000000000000e+02 1.0e+02 2.01e+00  5e-01  3e+00 1:28.9\n",
      "   31   9860 8.700000000000000e+01 1.3e+02 3.07e+00  7e-01  4e+00 1:44.6\n",
      "   36  11300 8.700000000000000e+01 2.3e+02 4.08e+00  1e+00  6e+00 1:57.9\n",
      "   42  13028 8.600000000000000e+01 3.5e+02 5.31e+00  1e+00  9e+00 2:14.1\n",
      "   48  14756 8.600000000000000e+01 4.7e+02 5.48e+00  1e+00  7e+00 2:29.8\n",
      "   51  15620 8.600000000000000e+01 6.5e+02 4.88e+00  9e-01  6e+00 2:37.3\n",
      "termination on tolfunhist=1e-12 after 5 restarts (Wed Dec  9 14:06:35 2020)\n",
      "final/bestever f-value = 8.900000e+01 8.600000e+01\n",
      "incumbent solution: [0.6494950033556806, 1.1454522240475944, 0.7249224889141684, -33.2145151990018, -12.588744439377194, 6.270136996439895]\n",
      "std deviation: [0.9383269840259888, 0.9521726767473561, 0.9661263241750013, 5.735261235530745, 2.2499396343527795, 0.9759414343214444]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Run simulation to test your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\").env\n",
    "env.seed(np.random.randint(1, 1000))\n",
    "env.reset()\n",
    "\n",
    "try:\n",
    "    for _ in range(1, niter+1):\n",
    "        sys.stdout.flush()\n",
    "        action = agent.act(env.state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update the visualization\n",
    "        env.render()\n",
    "\n",
    "        # check for rewards\n",
    "        if state[0] >= 0.5:\n",
    "            print(\"\\rTop reached at t = {}\".format(_))\n",
    "            break\n",
    "        elif  _ == niter:\n",
    "            print(\"\\rFailed to reach the top\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2. Continuous Action Spaces\n",
    "\n",
    "Unlike MountainCar v0, the action (engine force applied) is allowed to be a continuous value. The goal is to find optimal policy using Direct Search Algorithm while allowing continuous actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your import ?\n",
    "import cma\n",
    "\n",
    "class AgentContinuous:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Init a new agent.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn your policy.\n",
    "\n",
    "        Possible action: real\n",
    "        Range observation (tuple):\n",
    "            - position: [-1.2, 0.6]\n",
    "            - velocity: [-0.07, 0.07]\n",
    "        \"\"\"\n",
    "        \n",
    "        def objective_function(policy):\n",
    "            total = 0\n",
    "            env = gym.make(\"MountainCarContinuous-v0\")\n",
    "            env.seed(np.random.randint(1000))\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = [np.random.uniform(-1, 1)] # random action :/\n",
    "                state, reward, done, info = env.step(action)\n",
    "                total += -1\n",
    "            return - total # loss\n",
    "        \n",
    "        policy_opt, _ = cma.fmin2(objective_function, np.zeros(2), 0.5)\n",
    "\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Acts given an observation of the environment (using learned policy).\n",
    "\n",
    "        Takes as argument an observation of the current state, and\n",
    "        returns the chosen action.\n",
    "        See environment documentation: https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "        Possible action: real\n",
    "        Range observation (tuple):\n",
    "            - position: [-1.2, 0.6]\n",
    "            - velocity: [-0.07, 0.07]\n",
    "        \"\"\"\n",
    "        return [np.random.uniform(-1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_continuous = AgentContinuous()\n",
    "agent_continuous.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\").env\n",
    "env.seed(np.random.randint(1, 1000))\n",
    "env.reset()\n",
    "\n",
    "try:\n",
    "    for _ in range(1, niter+1):\n",
    "        sys.stdout.flush()\n",
    "        action = agent_continuous.act(env.state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update the visualization\n",
    "        env.render()\n",
    "\n",
    "        # check for rewards\n",
    "        if state[0] >= 0.5:\n",
    "            print(\"\\rTop reached at t = {}\".format(_))\n",
    "            break\n",
    "        elif  _ == niter:\n",
    "            print(\"\\rFailed to reach the top\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3 - Grading\n",
    "Run all cells and send output pdf to heri(at)lri(dot)fr before December, 9th 2020 at 23:59."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "√âditer les M√©ta-Donn√©es",
  "deletable": false,
  "editable": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "trusted": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
